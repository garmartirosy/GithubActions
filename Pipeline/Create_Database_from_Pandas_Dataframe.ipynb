{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garmartirosy/GithubActions/blob/main/Pipeline/Create_Database_from_Pandas_Dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project overview:  \n",
        "https://model.earth/OpenFootprint/trade"
      ],
      "metadata": {
        "id": "IF_syoaUCSb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCs71mIy90Mg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def infer_data_types(df, db_type='postgres'):\n",
        "    \"\"\"\n",
        "    Infers SQL data types for each column in a DataFrame based on its contents,\n",
        "    and adjusts the inferred type based on the database type (PostgreSQL or DuckDB).\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The DataFrame to infer data types for.\n",
        "    db_type (str): The type of database to infer types for ('postgres' or 'duckdb').\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary with DataFrame column names as keys and inferred SQL data types as values.\n",
        "    \"\"\"\n",
        "\n",
        "    inferred_types = {}\n",
        "\n",
        "    for column_name, column_data in df.items():\n",
        "        if pd.api.types.is_integer_dtype(column_data):\n",
        "            if db_type == 'postgres':\n",
        "                inferred_types[column_name] = 'INT'\n",
        "            elif db_type == 'duckdb':\n",
        "                inferred_types[column_name] = 'BIGINT'  # DuckDB uses BIGINT for integer types\n",
        "        elif pd.api.types.is_float_dtype(column_data):\n",
        "            if db_type == 'postgres':\n",
        "                inferred_types[column_name] = 'FLOAT'\n",
        "            elif db_type == 'duckdb':\n",
        "                inferred_types[column_name] = 'DOUBLE'  # DuckDB uses DOUBLE for floating point numbers\n",
        "        elif pd.api.types.is_bool_dtype(column_data):\n",
        "            inferred_types[column_name] = 'BOOLEAN'  # BOOLEAN is supported in both PostgreSQL and DuckDB\n",
        "        elif pd.api.types.is_datetime64_any_dtype(column_data):\n",
        "            inferred_types[column_name] = 'TIMESTAMP'  # TIMESTAMP is supported in both PostgreSQL and DuckDB\n",
        "        else:\n",
        "            # Handle string types with VARCHAR based on max length\n",
        "            max_length = column_data.astype(str).map(len).max()\n",
        "            if db_type == 'postgres':\n",
        "                inferred_types[column_name] = f'VARCHAR({2 * max_length})' if max_length > 0 else 'VARCHAR(255)'\n",
        "            elif db_type == 'duckdb':\n",
        "                inferred_types[column_name] = f'VARCHAR({2 * max_length})' if max_length > 0 else 'VARCHAR'\n",
        "\n",
        "    return inferred_types\n",
        "\n",
        "\n",
        "def generate_create_table_sql(table_name, df, db_type='postgres'):\n",
        "    \"\"\"\n",
        "    Generates a SQL CREATE TABLE statement from the DataFrame,\n",
        "    automatically inferring data types from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    table_name (str): Name of the table to be created.\n",
        "    df (pd.DataFrame): The DataFrame from which to infer data types and column names.\n",
        "    db_type (str): The type of database ('postgres' or 'duckdb').\n",
        "\n",
        "    Returns:\n",
        "    str: A SQL CREATE TABLE statement.\n",
        "    \"\"\"\n",
        "\n",
        "    # Infer data types for the DataFrame\n",
        "    inferred_types = infer_data_types(df, db_type=db_type)\n",
        "\n",
        "    # Start the CREATE TABLE statement\n",
        "    create_stmt = f\"CREATE TABLE {table_name} (\"\n",
        "\n",
        "    # Generate column definitions using DataFrame column names\n",
        "    column_definitions = [f\"{col} {inferred_types[col]}\" for col in df.columns]\n",
        "\n",
        "    # Join column definitions and complete the SQL statement\n",
        "    create_stmt += \", \".join(column_definitions) + \");\"\n",
        "    return create_stmt\n",
        "\n",
        "\n",
        "def generate_insert_sql(table_name, df, db_type='postgres'):\n",
        "    \"\"\"\n",
        "    Generates a SQL INSERT statement for a DataFrame, adjusting for PostgreSQL or DuckDB placeholder styles.\n",
        "\n",
        "    Args:\n",
        "    table_name (str): Name of the SQL table.\n",
        "    df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "    db_type (str): The type of database ('postgres' or 'duckdb').\n",
        "\n",
        "    Returns:\n",
        "    tuple: A SQL INSERT INTO statement and a list of lists of values for parameterized execution.\n",
        "    \"\"\"\n",
        "    # Determine placeholder based on database type\n",
        "    placeholder = '%s' if db_type == 'postgres' else '?'\n",
        "\n",
        "    # Use the DataFrame's column names for the INSERT statement\n",
        "    column_names = \", \".join(df.columns)\n",
        "    placeholders = \", \".join([placeholder for _ in df.columns])\n",
        "    insert_stmt = f\"INSERT INTO {table_name} ({column_names}) VALUES ({placeholders});\"\n",
        "\n",
        "    # Convert NaN to None for SQL NULL compatibility and prepare values for all rows\n",
        "    values = [\n",
        "        [value if pd.notnull(value) else None for value in row]\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "    return insert_stmt, values\n",
        "def create_and_insert_tables_final(cursor, table_name, df, db_type='postgres'):\n",
        "    \"\"\"\n",
        "    Creates a SQL table and inserts data from a DataFrame into it using bulk insertion.\n",
        "\n",
        "    Args:\n",
        "    cursor: The cursor to execute SQL commands.\n",
        "    table_name (str): The name of the SQL table.\n",
        "    df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "    db_type (str): The type of database ('postgres' or 'duckdb').\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Generate and execute the CREATE TABLE statement\n",
        "    try:\n",
        "        create_table_sql = generate_create_table_sql(table_name, df, db_type=db_type)\n",
        "        cursor.execute(create_table_sql)\n",
        "        print(f\"Table '{table_name}' created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating table '{table_name}': {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Generate the INSERT statement and values\n",
        "    try:\n",
        "        insert_stmt, insert_values = generate_insert_sql(table_name, df, db_type=db_type)\n",
        "\n",
        "        # Step 3: Execute the INSERT statement using executemany for bulk insertion\n",
        "        cursor.executemany(insert_stmt, insert_values)\n",
        "        print(f\"Data inserted successfully into '{table_name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error inserting data into '{table_name}': {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import psycopg2\n",
        "from io import StringIO  # Import StringIO for in-memory file operations\n",
        "import time  # Import time to add delays if needed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Supabase credentials - Replace with your credentials\n",
        "supabase_url = \"https://mfckwbbvsijzyckpzlno.supabase.co\"\n",
        "supabase_key = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im1mY2t3YmJ2c2lqenlja3B6bG5vIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzEzNzIyNzIsImV4cCI6MjA0Njk0ODI3Mn0.tpvsRq4BpUf4ycCX_IRBzPpa71JTh0xOjZyHCcuANIU\"\n",
        "\n",
        "# PostgreSQL connection details from Supabase (find this in your Supabase dashboard under Settings > Database)\n",
        "db_host = \"aws-0-us-west-1.pooler.supabase.com\"\n",
        "db_name = \"postgres\"\n",
        "db_user = \"postgres.mfckwbbvsijzyckpzlno\"\n",
        "db_password = \"ModelEarth2@123\"\n",
        "db_port = \"6543\"\n",
        "\n",
        "# Function to drop a table\n",
        "\n",
        "data = {\n",
        "    'order_id': [1, 2, 3, 4, 5],\n",
        "    'product_price': [1.1, 2.2, 3.3, 4.4, 5.5],\n",
        "    'is_available': [True, False, True, False, True],\n",
        "    'purchase_date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']),\n",
        "    'product_name': ['apple', 'banana', 'cherry', 'date', 'elderberry'],\n",
        "    'misc_data': [1, 'text', 3.14, True, None]\n",
        "}\n",
        "\n",
        "df = pd.read_feather(\"/content/IOT_2018_ixi.feather\")\n",
        "\n",
        "conn = psycopg2.connect(\n",
        "    host=db_host,\n",
        "    database=db_name,\n",
        "    user=db_user,\n",
        "    password=db_password,\n",
        "    port=db_port\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    cur = conn.cursor()\n",
        "    create_and_insert_tables_final(cur, 'tbl11', df, db_type='postgres')\n",
        "    conn.commit()\n",
        "    cur.close()\n",
        "except Exception as e:\n",
        "    print(f\"Database operation failed: {e}\")\n",
        "finally:\n",
        "    conn.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKH5Ku5wdcMq",
        "outputId": "951e3292-fa0a-4554-d31a-b9d58cd708ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error creating table 'tbl11': syntax error at or near \"IN\"\n",
            "LINE 1: ..., US INT, JP INT, CN INT, CA INT, KR INT, BR INT, IN INT, MX...\n",
            "                                                             ^\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "roohu0vWdzoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}